# K-近邻算法

## 算法思想

对于未知类别属性数据集中的点：
* 计算已知类别数据集中的点与当前点的距离
* 按照距离依次排序
* 选取与当前点距离最小的K个点
* 确定前K个点所在类别的出现概率
* 返回前K个点出现频率最高的类别作为当前点预测分类。

## 概述

KNN算法本身简单有效，它是一种lazy-learning算法。
分类器**不需要使用训练集进行训练**，训练时间复杂度为0.
KNN分类的计算复杂度和训练集中的文档数目成正比，也就是说，如果训练集中文档总数为n,那么KNN的分类时间复杂度为O(n)。

## 基本要素

* K值的选择

* 距离度量

* 分类决策

## 存在的问题

该算法在分类时有个主要的不足：当样本不平衡时，如一个类别的容量很大，而其他类的样本容量很小时，有可能导致当输入一个新样本时，该样本的K个令居中大容量类的样本占多数，也就是说，很容易由于样本本身的分类不平衡，导致最终在分类的时候，直接将新样本划分到容量比较大的类别当中。

## 解决思路

不同的样本给予不同的权重项。

## 超参数

在训练过程中超参数可以用不同的方式来表示

## 交叉验证

在我们建立模型的过程中，我们可以通过交叉验证的方式来确定一些参数，使得模型的建立更加准确。
