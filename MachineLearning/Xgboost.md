# Xgboost

## 集成算法思想

集成算法表示的是，将一些弱的分类器组合起来，表达出一种比较强的功能。

## 算法

* yi = ∑wj*xij 预测值等于多个树中叶子节点处的值与权重的乘积之和

* 目标函数：l(real_y,yi) = (real_y-yi)^2，误差。

* 对于以上目标函数，每一个测试数据都会得到一个与真实值之间的误差的平方，我们所以我们的目标就是要让误差平方所得到的值的均值达到最小。

* 惩罚函数：`Ω(ft) = γT + (1/2)λ∑(ωj)^2` ,其中T表示树的个数，后边的求和也是在求每个树下权重，γ表示惩罚力度。

* 最终的目标函数(损失函数)：F = 误差的平方的均值 + Ω(ft)

* `F = ∑l(real_y,y^(t-1) + ft(xi)) + Ω(ft) + constant`  其中`(real_y,y^(t-1) + ft(xi))`表示的是第t颗树(第t个分类器)加进来之后，t个分类器的效果等于前面(t-1)个分
类器的效果。

## 基本原理

`F = ∑(yi - (y^(t-1) + ft(xi)))^2 + Ω(ft) + constant`----`F = ∑[2(yi^(t-1) - real_y)ft(xi) + ft(xi)^2] + Ω(ft) + constant`

在构造新的一个分类器的时候，Xgboost都会将前面所有的分类器当成一个整体，然后通过前面的整体预测的值与真实值之间计算一个残差，根据残差的值来构造新的分类器，从而使得整体模型
的预测效果达到最好！

## 使用方法

* 对损失函数进行泰勒级数展开，再进行化简，我们可以得到这样的一个目标函数`∑[Gjwj + (1/2)(Hi+λ)wj^2] + γT` 其中`Gj`表示每一个叶子节点中的样本的一阶倒数的和，`Hi`表示每二个叶子节点中的样本的一阶倒数的和
我们要使得目标函数的值最小。

* 经过上边的计算，我们可以将目标函数化简为`Obj = (-1/2)∑(Gj^2/(Hj + λ)) + γT`

* Gain：我们最终得到了一种衡量标准，类似于熵和gini系数。
