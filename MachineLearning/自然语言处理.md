# 自然语言处理

自然语言处理的作用：智能客服、机器翻译、以及文章错误纠正等好多方面。

下面，我们来谈一谈普通的自然语言处理对于一句话的识别中存在的问题：

## 普通自然语言处理

句子假设就是：**我们今天来研究自然语言处理**

这句话出现的概率`P(S) = p(w1)*p(w2|w1)*p(w3|w1,w2)*...*p(wn|w1,w2,w3...,wn-1)`

也就是说，这句话出现的概率取决于组成这句话的所有的词语出现的概率的累乘，而每个词出现的概率又总是依赖于它前面出现的所有词语的概率，这样会导致两个比较严重的问题：

* 数据过于稀疏

* 参数空间太大

## N-gram模型

在这种模型当中，我们认为一个词的出现，它并不一定于它前面的所有词都相关，而是只与它前面的一个词相关或者只与它前面的两个词相关

**只与它前面的一个词相关**：`P(S) = p(w1)*p(w2|w1)*p(w3|w2)*...*p(wn|wn-1)`

**与它前面的两个词相关**：`P(S) = p(w1)*p(w2|w1)*p(w3|w1,w2)*...*p(wn|wn-1,wn-2)`

N-gram模型当中的N就是用来限定一个词的出现，到底和它前面的几次词相关

N-gram模型当中的模型的参数量级是(O(N^n)),其中N表示词典(语料库)的大小，n指的是指定一个词与它前面的几个词相关


## 神经网络模型

### 词向量(word2vec)

在将词转化成向量的过程中，我们关心更多的是词本身的逻辑（或者整体的逻辑），而不是它的表现形式。

这种模型的结构分为输入层-->投影层(用于将输入层输入的内容进行拼接)-->隐藏层-->输出层

将文字(词)转化成向量的形式，所以在这种模型当中，我们不仅要通过训练来迭代最终的权重参数，我们还要进行从输入层到投影层的优化，最终将输入转化为词向量。

### Hierarchical Softmax

Hierarchical Softmax就是利用哈夫曼树的逻辑来构建模型进行二分类，解决实际问题。

#### CBOW(Continuous Bag-of-Words Model)

是一种**根据上下文，预测当前的词**的出现概率的模型，输入为上下文
`L = ∑log p(w|Context(w))`，目标函数，其值越大越好。

* 输入层：是上下文的词语的词向量，在训练CBOW模型时，词向量只是一个副产品，我们真正要得到的是参数(权重)，在训练迭代的过程中，我们不仅要像普通的机器学习算法一样不断地优化参数，还要不断优化词向量中的每个值。

* 投影层：对词向量进行求和，求和就是普通的向量加法。

* 输出层：输出最可能的w。

具体要做的：

  * 对参数进行更新θ;

  * 对上下文进行更新Xw，使上下文越来越准确，这里的Xw不是一个个单独的X，而是经过投影层，连接在一起的X;

  * 对于每一个具体的X，我们是通过将对Xw的更新应用到每一个具体的X上去，做了这样的一个近似。也就是说，所有的具体的词向量的跟新的速率是由Xw来决定的。这样做，使得所有的词向量可以朝着同一个方向去更新。

求解过程中，用到了Logic回归模型即sigmoid函数，方法：梯度上升的方法。

#### Skip-gram

**已知当前的词，预测上下文**

输入为当前的词


### Negative Sampling(负采样)

这种方法的解决思路，主要是这样的：同样在已知上下文的情况下，我们预测当前的这个词，预测的是对的，则取值为1，如果预测的是错的，则取值为0。除此之外，要保证频次越高的样本越容易被采样。

Hierarchical Softmax要不断进行二分类，所以整体做起来会比较复杂，而Negative Sampling做起来会相对简单一些。

目标函数：`g(x) = σ(θXw)*∏(1-σ(θX))`,其中`σ(θXw)`表示正确的概率，`1-σ(θX)`表示错误的概率。

它的求解过程与`Hierarchical Softmax`中的`CBOW`的求解思路是一样的：

* 对参数进行更新-->θ;

* 对上下文进行更新-->Xw，使上下文越来越准确，这里的Xw不是一个个单独的X，而是经过投影层，连接在一起的X;

* 对于每一个具体的X，我们是通过将对Xw的更新应用到每一个具体的X上去，做了这样的一个近似。也就是说，所有的具体的词向量的跟新的速率是由Xw来决定的。这样做，使得所有的词向量可以朝着同一个方向去更新。
