# 自然语言处理

自然语言处理的作用：智能客服、机器翻译、以及文章错误纠正等好多方面。

下面，我们来谈一谈普通的自然语言处理对于一句话的识别中存在的问题：

## 普通自然语言处理

句子假设就是：**我们今天来研究自然语言处理**

这句话出现的概率`P(S) = p(w1)*p(w2|w1)*p(w3|w1,w2)*...*p(wn|w1,w2,w3...,wn-1)`

也就是说，这句话出现的概率取决于组成这句话的所有的词语出现的概率的累乘，而每个词出现的概率又总是依赖于它前面出现的所有词语的概率，这样会导致两个比较严重的问题：

* 数据过于稀疏

* 参数空间太大

## N-gram模型

在这种模型当中，我们认为一个词的出现，它并不一定于它前面的所有词都相关，而是只与它前面的一个词相关或者只与它前面的两个词相关

**只与它前面的一个词相关**：`P(S) = p(w1)*p(w2|w1)*p(w3|w2)*...*p(wn|wn-1)`

**与它前面的两个词相关**：`P(S) = p(w1)*p(w2|w1)*p(w3|w1,w2)*...*p(wn|wn-1,wn-2)`

N-gram模型当中的N就是用来限定一个词的出现，到底和它前面的几次词相关

N-gram模型当中的模型的参数量级是(O(N^n)),其中N表示词典(语料库)的大小，n指的是指定一个词与它前面的几个词相关


## 神经网络模型

### 词向量(word to vector)

在将词转化成向量的过程中，我们关心更多的是词本身的逻辑（或者整体的逻辑），而不是它的表现形式。

这种模型的结构分为输入层-->投影层(用于将输入层输入的内容进行拼接)-->隐藏层-->输出层

将文字(词)转化成向量的形式，所以在这种模型当中，我们不仅要通过训练来迭代最终的权重参数，我们还要进行从输入层到投影层的优化，最终将输入转化为词向量。

### 实现(Hierarchical Softmax)

* #### **CBOW(Continuous Bag-of-Words Model)**

是一种**根据上下文，预测当前的词**的出现概率的模型，输入为上下文
`L = ∑log p(w|Context(w))`

* #### **Skip-gram**

**已知当前的词，预测上下文**

输入为当前的词
