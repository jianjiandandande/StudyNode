---
title: SVD原理
tags: 机器学习 算法
grammar_cjkRuby: true
---


# SVD奇异值分解

## 原理

`A(m*n) = U(m*m)S(对角矩阵m*n)V(n*n)`

* 分解之后的对角矩阵S中的值(特征值)是按照从大到小的顺序进行排列的，这些值代表了特征向量的重要程度。值越大，对应的特征向量的重要程度越大。

* S矩阵中的特征值所对应的特征向量就是这个矩阵的变化方向。

如果说一个向量v是方阵A的特征向量，将一定可以表示成下面的形式：

`Av = λv`向量乘以矩阵之后的结果，相当于对向量做了一个线性的变换。

如果我们要描述好一个变换，那我们就描述好这个变换主要的变化方向就好了。

在上边讨论的基础上，如果我们只想讨论S中前r个最重要的特征，那么我们可以将矩阵A转化为`A(m*n) = U(m*r) S(r*r) V(r*n)`,其中`r<<m,n`

## 作用

SVD相当于把咱们原始的输入的样本，进行了特征的压缩，压缩除了不同的特征向量，以及中间的对角矩阵，来衡量特征向量的重要性。
