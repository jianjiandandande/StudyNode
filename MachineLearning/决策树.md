# 决策树

决策树，既可以用来处理分类问题，也可以用来处理回归的问题

## 决策树的影响因素

### 熵

熵一般用来反映一种混乱程度，混乱程度越高，熵值越大，相应的纯度就越低

### gini系数

gini系数和熵表达的意义相同，gini系数越大，混乱程度就越高，初度越高，gini系数就越小


决策树的基本想法是随着树深度的增加，节点的熵迅速地降低。熵降低的速度越快越好，这样我们就可以得到一颗高度最矮的决策树。

### 信息增益(gain)---ID3算法

在默认情况下，没有分类的情况下，事物本身的混乱程度，即它的熵值为H0,
根据某一个属性值对他进行分类后，事物此时的熵H1相对于上一次没有分类时的H0有所下降，也就是分类提高了事物的纯度，混乱度下降
### 信息增益率(C4.5)
此时，信息增益 = H1 - H0


信息增益率 = 信息增益/自身的信息增益(自身的信息增益指的是根据当前属性分类之后，类别之间的混乱度)


## 目标函数

评价函数 C(T)=∑N·H(t)(它越小越好，即越纯越好)算的是叶子节点的C(T),N表示该叶子节点中数据的个数


## 剪枝操作

因为我们最终想要得到的是一颗高度最矮的决策树，也就是说，对于一个数据，我们在对他进行分类的过程中，没必要使得每一个叶子节点处的熵值都为0
也就是说我们不需要每个叶子节点的纯度达到最高，因为如果纯度达到太高，会出现过拟合的问题，会导致最终训练出来的模型有噪音，所以我们应该对这
类问题进行处理

* 预剪枝：在构建决策树的过程时，提前停止(可以预先考虑最终决策树的深度，或者子节点处数据的数量等)

* 后剪枝：决策树构建好，然后才开始裁剪
  Cα(T) = C(T)+α·|Tleaf|(损失函数，其中C(T)表示上边提到的评价函数，Tleaf表示叶子节点的个数)--叶子节点个数越多，损失越大
